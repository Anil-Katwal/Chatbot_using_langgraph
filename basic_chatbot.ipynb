{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa94b33",
   "metadata": {},
   "source": [
    "# Advanced Chatbot using LangGraph with Groq + RAG\n",
    "\n",
    "This notebook demonstrates a sophisticated chatbot that supports:\n",
    "- Normal conversations using Groq's fast LLM\n",
    "- RAG (Retrieval-Augmented Generation) for accurate information retrieval\n",
    "- Document upload functionality for external documents\n",
    "- Memory and persistence to maintain context across sessions\n",
    "- LangSmith integration for tracing and debugging\n",
    "- User-friendly interface\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install langgraph langchain langchain-groq langsmith chromadb sentence-transformers pypdf pymupdf python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41cb83f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Dict, List, Any, TypedDict, Annotated\n",
    "from datetime import datetime\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader, \n",
    "    TextLoader, \n",
    "    Docx2txtLoader,\n",
    "    UnstructuredFileLoader\n",
    ")\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "import langsmith\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a8e6012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured!\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables\n",
    "# You'll need to set these in your environment or use a .env file\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"advanced-chatbot-groq-rag\"\n",
    "\n",
    "# Set your Groq API key\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"\n",
    "\n",
    "# Optional: Set your LangSmith API key for tracing\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-api-key-here\"\n",
    "\n",
    "print(\"Environment configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc89e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State structure defined!\n"
     ]
    }
   ],
   "source": [
    "class ChatState(TypedDict):\n",
    "    \"\"\"State for the chatbot conversation\"\"\"\n",
    "    messages: Annotated[List[Any], \"The conversation messages\"]\n",
    "    user_id: Annotated[str, \"The user ID for session management\"]\n",
    "    context: Annotated[Dict[str, Any], \"Additional context for the conversation\"]\n",
    "    retrieved_docs: Annotated[List[str], \"Retrieved documents for RAG\"]\n",
    "    current_step: Annotated[str, \"Current processing step\"]\n",
    "\n",
    "print(\"State structure defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc04e716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq model initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq language model\n",
    "# You can choose from different Groq models:\n",
    "# - \"llama3-8b-8192\" (fast, good for chat)\n",
    "# - \"llama3-70b-8192\" (slower but more capable)\n",
    "# - \"mixtral-8x7b-32768\" (good balance)\n",
    "# - \"gemma2-9b-it\" (fast and efficient)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",  # Fast and good for chat\n",
    "    temperature=0.7,\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"Groq model initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4195c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loading functions defined!\n"
     ]
    }
   ],
   "source": [
    "def load_document(file_path: str):\n",
    "    \"\"\"Load document based on file extension\"\"\"\n",
    "    file_extension = file_path.lower().split('.')[-1]\n",
    "    \n",
    "    try:\n",
    "        if file_extension == 'pdf':\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_extension == 'txt':\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_extension == 'docx':\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            # Try unstructured loader for other file types\n",
    "            loader = UnstructuredFileLoader(file_path)\n",
    "        \n",
    "        documents = loader.load()\n",
    "        print(f\"Successfully loaded {len(documents)} pages/sections from {file_path}\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_from_directory(directory_path: str):\n",
    "    \"\"\"Load all supported documents from a directory\"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Supported file extensions\n",
    "    supported_extensions = ['*.pdf', '*.txt', '*.docx', '*.md']\n",
    "    \n",
    "    for extension in supported_extensions:\n",
    "        file_pattern = os.path.join(directory_path, extension)\n",
    "        files = glob.glob(file_pattern)\n",
    "        \n",
    "        for file_path in files:\n",
    "            documents = load_document(file_path)\n",
    "            all_documents.extend(documents)\n",
    "    \n",
    "    print(f\"Total documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "print(\"Document loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b4941ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample knowledge base created!\n"
     ]
    }
   ],
   "source": [
    "# Sample knowledge base (you can replace this with your own documents)\n",
    "sample_docs = [\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combines retrieval with text generation for more accurate responses.\",\n",
    "    \"LangSmith is a platform for debugging, testing, evaluating, and monitoring LLM applications.\",\n",
    "    \"Memory in chatbots helps maintain context across conversation turns.\",\n",
    "    \"Vector databases store embeddings for efficient similarity search.\",\n",
    "    \"LangChain provides building blocks for LLM applications.\",\n",
    "    \"Groq is a fast LLM inference platform that provides ultra-low latency responses.\",\n",
    "    \"Groq's models include Llama, Mixtral, and Gemma variants optimized for speed.\",\n",
    "    \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\",\n",
    "    \"Natural language processing (NLP) helps computers understand and generate human language.\",\n",
    "    \"Deep learning uses neural networks with multiple layers to solve complex problems.\",\n",
    "    \"Data science combines statistics, programming, and domain expertise to extract insights from data.\",\n",
    "    \"Cloud computing provides on-demand access to computing resources over the internet.\",\n",
    "    \"APIs (Application Programming Interfaces) allow different software systems to communicate with each other.\"\n",
    "]\n",
    "\n",
    "print(\"Sample knowledge base created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43fcc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store and retriever created successfully!\n",
      "Initial knowledge base contains 15 chunks\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and vector store\n",
    "try:\n",
    "    # Use a simple, reliable embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}  # Use CPU to avoid GPU issues\n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, \n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    # Create documents from sample text\n",
    "    sample_documents = text_splitter.create_documents(sample_docs)\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        sample_documents, \n",
    "        embeddings,\n",
    "        collection_name=\"chatbot_knowledge\"\n",
    "    )\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    \n",
    "    print(\"Vector store and retriever created successfully!\")\n",
    "    print(f\"Initial knowledge base contains {len(sample_documents)} chunks\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create vector store: {e}\")\n",
    "    print(\"The chatbot will work without RAG functionality.\")\n",
    "    retriever = None\n",
    "    vectorstore = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02340ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_to_knowledge_base(file_paths: List[str]):\n",
    "    \"\"\"Add uploaded documents to the knowledge base\"\"\"\n",
    "    global vectorstore, retriever\n",
    "    \n",
    "    if vectorstore is None:\n",
    "        print(\"Vector store not initialized. Cannot add documents.\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        all_documents = []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500, \n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        \n",
    "        # Load documents from files\n",
    "        for file_path in file_paths:\n",
    "            if os.path.isfile(file_path):\n",
    "                documents = load_document(file_path)\n",
    "                all_documents.extend(documents)\n",
    "            elif os.path.isdir(file_path):\n",
    "                documents = load_documents_from_directory(file_path)\n",
    "                all_documents.extend(documents)\n",
    "        \n",
    "        if not all_documents:\n",
    "            print(\"No documents were loaded successfully.\")\n",
    "            return False\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        split_documents = text_splitter.split_documents(all_documents)\n",
    "        \n",
    "        # Add to vector store\n",
    "        vectorstore.add_documents(split_documents)\n",
    "        \n",
    "        # Update retriever\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 3}\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully added {len(split_documents)} document chunks to knowledge base\")\n",
    "        print(f\"Total knowledge base size: {vectorstore._collection.count()} chunks\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error adding documents to knowledge base: {e}\")\n",
    "        return False\n",
    "\n",
    "def list_knowledge_base_info():\n",
    "    \"\"\"List information about the current knowledge base\"\"\"\n",
    "    if vectorstore is None:\n",
    "        print(\"No vector store available.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        count = vectorstore._collection.count()\n",
    "        print(f\"Knowledge base contains {count} document chunks\")\n",
    "        \n",
    "        # Get some sample documents\n",
    "        sample_docs = vectorstore.similarity_search(\"\", k=5)\n",
    "        print(f\"Sample documents in knowledge base:\")\n",
    "        for i, doc in enumerate(sample_docs[:3], 1):\n",
    "            print(f\"{i}. {doc.page_content[:100]}...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting knowledge base info: {e}\")\n",
    "\n",
    "print(\"Document upload functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26fbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_use_rag(state: ChatState) -> str:\n",
    "    \"\"\"Determine if we should use RAG based on the user's question\"\"\"\n",
    "    if retriever is None:\n",
    "        return \"chat\"  # Fallback to chat if no retriever\n",
    "    \n",
    "    last_message = state[\"messages\"][-1].content.lower()\n",
    "    \n",
    "    # Keywords that suggest the user wants factual information\n",
    "    rag_keywords = [\n",
    "        \"what is\", \"how does\", \"explain\", \"tell me about\", \n",
    "        \"information\", \"knowledge\", \"define\", \"describe\",\n",
    "        \"what are\", \"how to\", \"guide\", \"tutorial\", \"find\",\n",
    "        \"search\", \"look for\", \"what does\", \"where is\"\n",
    "    ]\n",
    "    \n",
    "    # Check if the message contains RAG keywords\n",
    "    if any(keyword in last_message for keyword in rag_keywords):\n",
    "        return \"rag\"\n",
    "    else:\n",
    "        return \"chat\"\n",
    "\n",
    "print(\"RAG decision function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(state: ChatState) -> ChatState:\n",
    "    \"\"\"Retrieve relevant documents for RAG\"\"\"\n",
    "    if retriever is None:\n",
    "        state[\"retrieved_docs\"] = []\n",
    "        state[\"current_step\"] = \"no_retriever\"\n",
    "        return state\n",
    "    \n",
    "    try:\n",
    "        query = state[\"messages\"][-1].content\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        \n",
    "        state[\"retrieved_docs\"] = [doc.page_content for doc in docs]\n",
    "        state[\"current_step\"] = \"retrieved_context\"\n",
    "        \n",
    "        print(f\"Retrieved {len(state['retrieved_docs'])} relevant documents\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        state[\"retrieved_docs\"] = []\n",
    "        state[\"current_step\"] = \"retrieval_error\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Context retrieval function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006f87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_response(state: ChatState) -> ChatState:\n",
    "    \"\"\"Generate response using RAG\"\"\"\n",
    "    if not state[\"retrieved_docs\"]:\n",
    "        # Fallback to chat if no documents retrieved\n",
    "        return generate_chat_response(state)\n",
    "    \n",
    "    context = \"\\n\".join(state[\"retrieved_docs\"])\n",
    "    query = state[\"messages\"][-1].content\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question accurately and informatively. Context: {context} If the context doesn't contain relevant information, say so politely and provide a general helpful response.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    try:\n",
    "        response = chain.invoke({\n",
    "            \"context\": context,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        state[\"messages\"].append(AIMessage(content=response))\n",
    "        state[\"current_step\"] = \"completed\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {e}\")\n",
    "        # Fallback to chat\n",
    "        return generate_chat_response(state)\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"RAG response function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_response(state: ChatState) -> ChatState:\n",
    "    \"\"\"Generate conversational response\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a friendly and helpful AI assistant powered by Groq. Engage in natural conversation with the user. Be informative, helpful, and engaging.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke({\n",
    "        \"messages\": state[\"messages\"]\n",
    "    })\n",
    "    \n",
    "    state[\"messages\"].append(AIMessage(content=response))\n",
    "    state[\"current_step\"] = \"completed\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"Chat response function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(ChatState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"should_use_rag\", should_use_rag)\n",
    "workflow.add_node(\"retrieve_context\", retrieve_context)\n",
    "workflow.add_node(\"generate_rag_response\", generate_rag_response)\n",
    "workflow.add_node(\"generate_chat_response\", generate_chat_response)\n",
    "\n",
    "# Add conditional edges from START\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    should_use_rag,\n",
    "    {\n",
    "        \"rag\": \"retrieve_context\",\n",
    "        \"chat\": \"generate_chat_response\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add edges for RAG path\n",
    "workflow.add_edge(\"retrieve_context\", \"generate_rag_response\")\n",
    "workflow.add_edge(\"generate_rag_response\", END)\n",
    "\n",
    "# Add edges for chat path\n",
    "workflow.add_edge(\"generate_chat_response\", END)\n",
    "\n",
    "# Compile the graph with memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"LangGraph workflow with RAG created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_bot(user_input: str, user_id: str = \"default_user\", config: Dict = None) -> str:\n",
    "    \"\"\"Main chat function\"\"\"\n",
    "    if config is None:\n",
    "        config = {\"configurable\": {\"thread_id\": user_id}}\n",
    "    \n",
    "    # Create initial state\n",
    "    state = {\n",
    "        \"messages\": [HumanMessage(content=user_input)],\n",
    "        \"user_id\": user_id,\n",
    "        \"context\": {},\n",
    "        \"retrieved_docs\": [],\n",
    "        \"current_step\": \"start\"\n",
    "    }\n",
    "    \n",
    "    # Run the workflow\n",
    "    result = app.invoke(state, config)\n",
    "    \n",
    "    # Return the last AI message\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "print(\"Chat interface ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a20b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Upload Instructions\n",
    "print(\"Document Upload Instructions:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"To add your own documents to the knowledge base:\")\n",
    "print(\"1. Place your documents in the same directory as this notebook\")\n",
    "print(\"2. Supported formats: PDF, TXT, DOCX, MD\")\n",
    "print(\"3. Use the add_documents_to_knowledge_base() function\")\n",
    "print(\"4. Example: add_documents_to_knowledge_base(['document.pdf', 'notes.txt'])\")\n",
    "print(\"5. Use list_knowledge_base_info() to see current knowledge base status\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example of how to add documents\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"# add_documents_to_knowledge_base(['your_document.pdf'])\")\n",
    "print(\"# list_knowledge_base_info()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chatbot with RAG\n",
    "print(\"Advanced Chatbot Demo with Groq + RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example conversations to test both RAG and chat\n",
    "test_questions = [\n",
    "    \"Hello! How are you today?\",  # Chat\n",
    "    \"What is LangGraph?\",  # RAG\n",
    "    \"How does RAG work?\",  # RAG\n",
    "    \"Tell me about Groq\",  # RAG\n",
    "    \"What's the weather like?\",  # Chat\n",
    "    \"Can you help me with a coding problem?\",  # Chat\n",
    "    \"Explain machine learning\",  # RAG\n",
    "    \"What is Python?\",  # RAG\n",
    "    \"How are you feeling?\",  # Chat\n",
    "    \"Tell me about APIs\"  # RAG\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"User: {question}\")\n",
    "    try:\n",
    "        response = chat_with_bot(question)\n",
    "        print(f\"Bot: {response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(\"-\" * 60)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f854569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Interactive Chat Interface with Groq + RAG</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Ask questions about uploaded documents or have a general conversation:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65258dbcf6d4485db601cc81a32593db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', layout=Layout(width='80%'), placeholder='Type your message here...'), Button(butâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af8927943dc4f19bf4b2ad217081bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive chat interface ready! Type your message and press Enter or click Send.\n",
      "Try asking questions about your uploaded documents or general questions like 'What is LangGraph?'\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Create the chat interface\n",
    "chat_output = widgets.Output()\n",
    "input_box = widgets.Text(placeholder='Type your message here...', layout=widgets.Layout(width='80%'))\n",
    "send_button = widgets.Button(description='Send', button_style='primary')\n",
    "clear_button = widgets.Button(description='Clear', button_style='warning')\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def on_send_click(b):\n",
    "    user_message = input_box.value.strip()\n",
    "    if user_message:\n",
    "        with chat_output:\n",
    "            print(f\"You: {user_message}\")\n",
    "            \n",
    "            try:\n",
    "                # Get bot response\n",
    "                bot_response = chat_with_bot(user_message)\n",
    "                print(f\"Bot: {bot_response}\")\n",
    "                \n",
    "                # Store in history\n",
    "                chat_history.append({\"user\": user_message, \"bot\": bot_response})\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        input_box.value = ''\n",
    "\n",
    "def on_clear_click(b):\n",
    "    chat_output.clear_output()\n",
    "    chat_history.clear()\n",
    "    print(\"Chat history cleared!\")\n",
    "\n",
    "def on_enter_press(change):\n",
    "    if change['name'] == 'value' and change['new'].endswith('\\n'):\n",
    "        input_box.value = change['new'].rstrip('\\n')\n",
    "        on_send_click(None)\n",
    "\n",
    "send_button.on_click(on_send_click)\n",
    "clear_button.on_click(on_clear_click)\n",
    "input_box.observe(on_enter_press, names='value')\n",
    "\n",
    "# Display the interface\n",
    "display(HTML('<h3>Interactive Chat Interface with Groq + RAG</h3>'))\n",
    "display(HTML('<p>Ask questions about uploaded documents or have a general conversation:</p>'))\n",
    "display(widgets.HBox([input_box, send_button, clear_button]))\n",
    "display(chat_output)\n",
    "\n",
    "print(\"Interactive chat interface ready! Type your message and press Enter or click Send.\")\n",
    "print(\"Try asking questions about your uploaded documents or general questions like 'What is LangGraph?'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
